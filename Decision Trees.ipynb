{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f0761e",
   "metadata": {},
   "source": [
    "\n",
    "# Decision Trees\n",
    "\n",
    "This notebook contains sample code to classify digits using the MNIST Dataset and the solutions to **Chapter 6 assignment.**\n",
    "\n",
    "\n",
    "Like SVMs, Decision Trees are versatile Machine Learning algorithms that can per‐\n",
    "form both classification and regression tasks, and even multioutput tasks. They are\n",
    "powerful algorithms, capable of fitting complex datasets. For example, in Chapter 2\n",
    "you trained a DecisionTreeRegressor model on the California housing dataset, fit‐\n",
    "ting it perfectly (actually, overfitting it).\n",
    "Decision Trees are also the fundamental components of Random Forests (see Chap‐\n",
    "ter 7), which are among the most powerful Machine Learning algorithms available\n",
    "today.\n",
    "\n",
    "In this chapter we will start by discussing how to train, visualize, and make predic‐\n",
    "tions with Decision Trees. Then we will go through the CART training algorithm\n",
    "used by Scikit-Learn, and we will discuss how to regularize trees and use them for\n",
    "regression tasks. Finally, we will discuss some of the limitations of Decision Trees.\n",
    "\n",
    "\n",
    "\n",
    "**About Dataset:**\n",
    "\n",
    "The MNIST dataset contains 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "**Credit:**\n",
    "\n",
    "O'Reilly book Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d43186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a07513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d92275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b226d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654e05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85972cc",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    "\n",
    "Complete Ch. 6 Exercises 1, 3, 4 and 7, pp. 186-187, inclusive:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ca040",
   "metadata": {},
   "source": [
    "1. What is the approximate depth of a Decision Tree trained (without restrictions)\n",
    "on a training set with one million instances?\n",
    "\n",
    "3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\n",
    "max_depth?\n",
    "\n",
    "4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling\n",
    "the input features?\n",
    "\n",
    "7. Train and fine-tune a Decision Tree for the moons dataset by following these\n",
    "steps:\n",
    "a. Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset.\n",
    "b. Use train_test_split() to split the dataset into a training set and a test set.\n",
    "c. Use grid search with cross-validation (with the help of the GridSearchCV\n",
    "class) to find good hyperparameter values for a DecisionTreeClassifier.\n",
    "Hint: try various values for max_leaf_nodes.\n",
    "d. Train it on the full training set using these hyperparameters, and measure\n",
    "your model’s performance on the test set. You should get roughly 85% to 87%\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "122e4836",
   "metadata": {},
   "source": [
    "1. The depth of a well-balanced binary tree containing m leaves is equal to log2\n",
    "(m),2\n",
    "rounded up. A binary Decision Tree (one that makes only binary decisions, as is\n",
    "the case with all trees in Scikit-Learn) will end up more or less well balanced at\n",
    "the end of training, with one leaf per training instance if it is trained without\n",
    "restrictions. Thus, if the training set contains one million instances, the Decision\n",
    "Tree will have a depth of log2\n",
    "(106) ≈ 20 (actually a bit more since the tree will\n",
    "generally not be perfectly well balanced).\n",
    "\n",
    "\n",
    "3. If a Decision Tree is overfitting the training set, it may be a good idea to decrease\n",
    "max_depth, since this will constrain the model, regularizing it.\n",
    "\n",
    "4. Decision Trees don’t care whether or not the training data is scaled or centered;\n",
    "that’s one of the nice things about them. So if a Decision Tree underfits the train‐\n",
    "ing set, scaling the input features will just be a waste of time.\n",
    "\n",
    "For the solutions to exercises 7, please see the Jupyter notebooks available at\n",
    "https://github.com/ageron/handson-ml2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3e469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833310c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9917b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
